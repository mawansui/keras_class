# Класс для создания нейросетей с помощью Keras

Здесь представлена документация к классу, помогающему создавать и обучать модели нейросетей глубокого обучения с использованием библиотеки Keras.

## Начало работы

Перед началом работы потребуется установить следующие пакеты и библиотеки:
- TensorFlow
- Keras
- h5py

`TensorFlow` – рекомендуемый бэкэнд для обучения сетей, однако возможно использовать Theano и CNTK.

`Keras` – высокоуровневая абстрактная надстройка над разными бэкэндами.

Пакет `h5py` нужен для сохранения модели на диск с последующим её извлечением.

### Установка

Инструкция по установке `TensorFlow`: https://www.tensorflow.org/install/

Инструкция по установке `Keras`: https://keras.io/#installation

Инструкция по установке `h5py`: http://docs.h5py.org/en/latest/build.html

## Использование класса

Сначала класс нужно импортировать

```
from keras_mlp import Keras_MLP
```

Использование класса выполняется в согласии со следующим алгоритмом:
1. [Инициализация класса с требуемыми параметрами нейросети](#init)
- [Создание модели с заданными параметрами](#model)
- [Обучение этой модели](#fit)
- [Предсказание значений](#predict)
- [Сохранение / загрузка модели](#save)

### <a name="init">1. Инициализация класса</a>

Прежде всего, нужно создать экземпляр данного класса, передав ему все необходимые параметры.

Список возможных параметров:
- **task** [тип: `строка`; возможные значения: `"regression"`, `"classification"`, `"multiclass"`, `"default"`, либо добавленные самостоятельно]
  - Этот параметр определяет, для каких целей будет использоваться класс. В соответствии с указанной в `task` задаче подбираются:
    - Функция активации (activation function) на последнем слое нейросети
    - Функция ошибки (loss function) для всей нейросети
    - Метрики качества обучения (metrics)

    Все три параметра для каждой из возможных задач указаны в файле `choose_parameters.py`. В этот файл можно при необходимости дописать и другие задачи, присвоив им соответствующие требуемые значения трех параметров.
- **layer_sizes** [тип: `кортеж`]
  - В этом параметре указывается число нейронов на каждом **скрытом** слое. Размерности входного и выходного слоёв устанавливаются автоматически на основании размерностей тренировочных данных.

- **activations** [тип: `строка` или `список`; возможные значения: `auto`, `название одной функции активации`, `[список функций активации с размерностью layer_sizes]`]
  - В этом параметре указывается функция активации для каждого скрытого слоя.
    - `"auto"`: после каждого скрытого слоя, указанного в `layer_sizes`, будет добавлена функция ReLU
    - `название одной функции активации`: указанная (одна) функция активации будет использоваться после каждого скрытого слоя
    - `список функций активации`: если длина этого списка **совпадает** с длиной кортежа скрытых слоёв, после каждого скрытого слоя будет идти соответствующая функция активации. Если числа будут разными, будет выведена ошибка.

- **dropout** [тип: `string` либо `список целых чисел`]
  - Здесь указывается значение для слоя дропаута, который будет идти после каждого скрытого слоя. Значения ранжируются от 0 до 1 (выкл. и вкл. соответственно).
    - `"auto"`: после каждого скрытого слоя будет идти dropout-слой со значением 0.25, заданным по умолчанию в файле `define_dropout.py`.
    - `список с 1 целым числом`: после каждого скрытого слоя будет идти слой дропаута с таким значением, как это одно число в списке.
    - `список целых чисел`: если этот список по длине будет равен длине котрежа скрытых слоёв, после каждого скрытого слоя будет идти слой дропаута с заданным значением. Если нет – выдаст ошибку.

- **alpha** [тип: `float`]
  - Указывает величину параметра L2, предотвращающего оверфиттинг.

- **batch_size** [тип: `string` или `int`]
  - Указывает сколько тренировочных объектов будет показано нейросети до того, как произойдет изменение весов.
    - `auto`: автоматически присвоит `batch_size == 200`
    - `любое целое число`: сделает размер батча таким, как указанное число

- **learning_rate_init** [тип: `float`]
  - Указывает скорость обучения нейросети, контролирует размер шага при изменении весов.

- **epochs** [тип: `int`]
  - Указывает число эпох (сколько раз оптимизатор будет изменять веса).

- **shuffle** [тип: `boolean`]
  - Указывает, перемешивать ли образцы при каждом прогоне нейросети

- **loss_function** [тип: `строка`]
  - Указывает функцию ошибки (необходимо для компиляции нейросети). [Список доступных функций из документации.](https://keras.io/losses/).
  - Этот параметр **можно не указывать** при инициализации класса. Тогда возьмется значение по умолчанию для задачи, указанной в **task**.
  - Но если этот параметр всё-таки указать, то будет использоваться именно он, а не дефолтный.

- **metrics** [тип: `список строк`]
  - Это список, в котором содержатся [названия разных метрик, доступных в библиотеке Keras](https://keras.io/metrics/).
  - Этот параметр **можно не указывать** при инициализации класса. Тогда возьмется список по умолчанию для задачи, указанной в **task**.
  - Но если этот параметр всё-таки указать, то будет использоваться именно он, а не дефолтный.

- **verbose** [тип: `int`; возможные значения: `0`, `1`, `2`]
  - Указывает, выводить или нет информацию о каждом прогоне нейросети в консоль.
    - `0`: не выводить ничего
    - `1`: выводить только прогресс-бар
    - `2`: выводить подробную информацию о каждой эпохе

- **early_stopping** [тип: `boolean`]
  - Прекращать обучение или нет, если качество предсказаний не улучшается.

- **optimizer_name** [тип: `строка`]
  - Название выбранного [оптимизатора весов из стандартной библиотеки Keras](https://keras.io/optimizers/).

- Затем следуют абсолютно любые параметры, требуемые для выбранного оптимизатора.

Пример инициализации класса:
```
classifier = Keras_MLP(
                task="classification",
                layer_sizes=(100, 100, 100),
                activations = 'relu',
                dropout='auto',
                alpha=0.00001*(2**1),
                batch_size=200,
                learning_rate_init=0.001,
                epochs=15,
                shuffle=True,
                verbose=1,
                early_stopping=False,
                optimizer_name="adam",
                loss_function="binary_crossentropy",
                metrics=["binary_accuracy"],
                lr=0.001,
                beta_1 = 0.9,
                beta_2 = 0.999,
                epsilon=1e-08)
```

### <a name="model">2. Создание модели с требуемыми параметрами</a>

Создать чистую модель очень просто. Это делается с помощью метода `create_model(x_train_shape, y_train_shape)`, который принимает размерности тренировочных данных для подбора необходимого количества нейронов на первом и последнем уровне нейросети. Главное иметь нужные данные, или хотя бы точно знать их размерность. Имея созданный выше экземпляр класса, создадим новую модель:

`new_model = classifier.create_model(x_train.shape[1], y_train.shape[1])`

Полученная модель является стандартной моделью Keras и обладает всеми возможностями, [описанными в документации](https://keras.io/models/sequential/).

### <a name="fit">3. Обучение модели</a>

Имея модель, обучить её довольно просто. Нужно лишь вызвать для неё метод `fit()`, стандартный для всех моделей Keras. Этот метод принимает данные (иксы и игреки) в виде Numpy-массивов, а также некоторые значения из созданого экземпляра класса, см. пример:
```
new_model.fit(x_train,
              y_train,
              batch_size=classifier.batch_size,
              epochs=classifier.epochs,
              verbose=classifier.verbose,
              callbacks=classifier.used_callbacks,
              shuffle=classifier.shuffle)
```

Удобным представляется проведение дообучения модели на части данных. Согласно [утверждению автора библиотеки Keras](https://github.com/keras-team/keras/issues/4446), "многократное повторение вызова метода `fit()` дообучает модель". Поэтому становится возможно выполнение операции, аналогичной указанной в этом примере:
```
for i in range(0, 100):
    new_model.fit(x_train,
                  y_train,
                  batch_size=classifier.batch_size,
                  epochs=classifier.epochs,
                  verbose=classifier.verbose,
                  callbacks=classifier.used_callbacks,
                  shuffle=classifier.shuffle)
```

### <a name="predict">4. Предсказание значений</a>

Предсказание выполняется при помощи вызова у обученной модели метода `predict()`. Этот метод принимает Numpy-массив с тестовыми данными. Возвращает также Numpy-массив предсказаний.

```
predictions = new_model.predict(x_test)
```

Подробнее можно прочитать в [документации к Keras-моделям](https://keras.io/models/sequential/).

### <a name="save">5. Сохранение и загрузка обученной модели</a>

Однако при необходимости Keras-модели можно сохранять в отдельные файлы. К сожалению, использование модуля `pickle` для этого [невозможно](https://github.com/keras-team/keras/issues/789), и, более того, [не рекомендуется авторами библиотеки](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model). Последние, тем не менее, предлагают альтернативный метод сохранения и загрузки обученных моделей – с помощью библиотеки [`h5py`](http://docs.h5py.org/en/latest/build.html).

Установив её на свой компьютер, можно сохранять модели с помощью вызова у модели метода `save()`. Этот метод принимает единственный агрумент – название сохраняемого файла вместе с директорией в виде одной строки (обратите внимание на расширение `.h5` в конце!):

```
new_model.save('./example/models/trained_on_6_06_2018.h5')
```

Загрузка моделей выполняется так же просто, с помощью дополнительной функции `load_model()`, которую надо предварительно импортировать. Эта функция принимает один аргумент – путь до файла с моделью:
```
from keras.models import load_model

model = load_model('./example/models/trained_on_6_06_2018.h5')
```

Подробнее о сохранении и загрузке моделей можно прочитать в [документации Keras](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model).

## TODOs:
- [ ] Проверить в "боевом" режиме :)
